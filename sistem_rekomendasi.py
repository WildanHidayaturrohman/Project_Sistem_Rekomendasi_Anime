# -*- coding: utf-8 -*-
"""sistem_rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JxPhFBFKtrn2vFslQR6WSxHKXFiuiNrI

## **Import Library**
"""

pip install kagglehub[pandas-datasets]

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import kagglehub
from kagglehub import KaggleDatasetAdapter

import warnings
warnings.filterwarnings('ignore')

"""## **Data Loading**"""

# Set the path to the file you'd like to load
file_path = "anime.csv"

# Load the latest version
anime_df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "CooperUnion/anime-recommendations-database",
  file_path)

anime_df

"""**Penjelasan:**
Terlihat bahwa dataset dari anime.csv memiliki 12294 baris dan 7 kolom yakni anime_id, name, genre, type, episodes, rating dan members. Ini mengandung makna bahwa dataset pada anime.csv berjumlah 12294.
"""

# Set the path to the file you'd like to load
file_path = "rating.csv"

# Load the latest version
rating_df = kagglehub.load_dataset(
  KaggleDatasetAdapter.PANDAS,
  "CooperUnion/anime-recommendations-database",
  file_path)

rating_df

"""**Penjelasan:**
Terlihat bahwa dataset dari rating.csv memiliki 7813737 baris dan 3 kolom yakni user_id, anime_id dan rating. Ini mengandung makna bahwa dataset pada rating.csv berjumlah 7813737.

## **Univariate EDA**

#### **1. anime_df**
"""

anime_df.info()

"""**Penjelasan:**
Berdasarkan anime_df.info(), diperoleh bahwa fitur anime_id memiliki jumlah data sebanyak 12294 dengan type datanya adalah int64, name memiliki jumlah data sebanyak 12294 dengan type datanya adalah object, genre memiliki jumlah data sebanyak 12232 dengan type datanya adalah object, type memiliki jumlah data sebanyak 12269 dengan type datanya adalah object, episodes memiliki jumlah data sebanyak 12294 dengan type datanya adalah object, rating memiliki jumlah data sebanyak 12064 dengan type datanya adalah float64 dan members memiliki jumlah data sebanyak 12294 dengan type datanya adalah int64.
"""

anime_df.describe()

"""**Penjelasan:**
Berdasarkan hasil dari anime_df.describe, diperoleh bahwa nilai rating berada pada rentang 1.67 sampai 10. Ini berarti bahwa nilai maximal rating yang ada adalah 10.
"""

print("Jumlah nama unik di kolom 'type':", anime_df['type'].nunique())
print("Daftar nama unik di kolom 'type':", anime_df['type'].unique())

print("\nJumlah nama unik di kolom 'name':", anime_df['name'].nunique())

"""**Penjelasan:**
Terlihat bahwa nama atau judul anime secara keseluruhan adalah 12292 dan type anime adalah 6 type yakni Movie, TV, OVA, Special, Music, ONA dan ada pula baris yang kosong.
"""

genre_anime = anime_df['genre'].str.split(', ').explode().unique()
print("Total unik genre:", len(genre_anime))
print("Daftar genre yang ada secara keseluruhan:", list(genre_anime))

"""**Penjelasan:**
Terlihat bahwa ada 44 kategori genre, kita ketahui pula bahwa satu anime dapat memiliki beberapa genre yang dipisahkan oleh tanda ", " sesuai pada dataframe anime_df.

#### **2. rating_df**
"""

rating_df.info()

rating_df.describe()

"""**Penjelasan:**
Dari rating_df.info() dan rating_df.describe(), dapat diketahui bahwa rating.csv memiliki 3 fitur yakni user_id dengan type data int64 dan berjumlah 7813737, anime_id dengan type data int64 dan berjumlah 7813737 serta rating dengan type data int64 dan berjumlah 7813737. Pada fitur rating pula, diketahui bahwa rating terkecil dimulai dari -1(rating yang digeneralisasi jika pengguna menonton anime tersebut namun tidak memberikan rating) hingga 10.

## **Data Preparation**

### Pengecekan Missing Value dan Duplikat serta Penanganannya pada Masing-masing Data

#### **1. Pengecekan**
"""

print("Missing value di anime_df:")
print(anime_df.isnull().sum())

print("\nMissing value di rating_df:")
print(rating_df.isnull().sum())

print("\nDuplicate data di anime_df:")
print(anime_df.duplicated().sum())

print("\nDuplicate data di rating_df:")
print(rating_df.duplicated().sum())

"""**Penjelasan:**
Terlihat bahwa anime_df memiliki beberapa missing value dan 0 duplikat data. Terlihat pula bahwa rating_df tidak memiliki missing value dan terdapat 1 duplikat data.
"""

# Identify baris dengan missing values di 'genre', 'type', dan 'rating'
missing_genre = anime_df['genre'].isnull()
missing_type = anime_df['type'].isnull()
missing_rating = anime_df['rating'].isnull()

# Hitung baris dengan missing value di kedua kolom 'genre' dan 'type'
count_genre_type = anime_df[missing_genre & missing_type].shape[0]
print(f"Jumlah baris dengan missing value pada kolom 'genre' dan 'type': {count_genre_type}")

# Hitung baris dengan missing value di kedua kolom 'genre' dan 'rating'
count_genre_rating = anime_df[missing_genre & missing_rating].shape[0]
print(f"Jumlah baris dengan missing value pada kolom 'genre' dan 'rating': {count_genre_rating}")

# Hitung baris dengan missing value di kedua kolom 'type' dan 'rating'
count_type_rating = anime_df[missing_type & missing_rating].shape[0]
print(f"Jumlah baris dengan missing value pada kolom 'type' dan 'rating': {count_type_rating}")

# Hitung baris dengan missing value di ketiga kolom 'genre', 'type', dan 'rating'
count_all_three = anime_df[missing_genre & missing_type & missing_rating].shape[0]
print(f"Jumlah baris dengan missing value pada kolom 'genre', 'type', dan 'rating': {count_all_three}")

# Menghitung jumlah baris yang memiliki setidaknya satu missing value
rows_with_missing_values = anime_df[anime_df.isnull().any(axis=1)].shape[0]
print(f"\nJumlah baris di anime_df yang memiliki missing value: {rows_with_missing_values}")

"""**Penjelasan:**
Terlihat bahwa missing value pada setiap kolom ternyata memiliki beberapa irisan dengan kolom lain. Hal itu terlihat dari pengecekan dimana missing value pada kolom 'genre' dan 'type': 3, missing value pada kolom 'genre' dan 'rating': 15, missing value pada kolom 'type' dan 'rating': 25 dan missing value pada kolom 'genre', 'type', dan 'rating': 3. Berdasarkan jumlah keseluruhan missing value pada setiap kolom yakni rating 230, type 25 dan genre 62, baris-baris yang memiliki missing value hanya berjumlah 277. Baris-baris ini yang nantinya akan dilakukan penanganan.

#### **2. Penanganan**
"""

anime_df.dropna(axis = 0, inplace = True)
print("Missing value di anime_df:")
print(anime_df.isnull().sum())

rating_df.drop_duplicates(inplace=True)
print("\nDuplicate data di rating_df:")
print(rating_df.duplicated().sum())

"""**Penjelasan:**
Setelah dilakukan penanganan, maka missing value dari anime_df dan duplikat dari rating_df sudah 0.
"""

anime_df.info()

rating_df.info()

"""**Penjelasan:**
Setelah dilakukan penanganan pada anime_df dan rating_df, maka diperoleh jumlah data pada anime_df untuk setiap fiturnya adalah sebanyakn 12017 dan jumlah data pada rating_df untuk setiap fiturnya adalah 7813736.

Pada Univariate EDA sebelumnya, diketahui bahwa jumlah judul anime adalah 12292, sedangkan setelah dilakukan penanganan duplikat dan missing value, jumlah name atau judul anime pada anime_df berjumlah 12017. Hal ini memiliki sedikit keanehan, karena ketika jumlah judul 12292 dan dilakukan penanganan missing value sebanyak 277, maka seharusnya data akhir berjumlah 12015. Ini memiliki arti terdapat indikasi duplikat pada judul. Akan dilihat lebih lanjut untuk judul yang tidak unik.
"""

# Mencari nama anime yang muncul lebih dari satu kali
duplicate_names = anime_df[anime_df.duplicated(subset=['name'], keep=False)]

# Menampilkan seluruh isinya sesuai kolom-kolom di anime_df
print("Data dengan nama anime yang tidak unik:")
duplicate_names

"""**Penjelasan:**
Terlihat bahwa name yang sama sebanyak 2 name dengan masing-masing memiliki 1 name yang sama. Namun terlihat pula bahwa type dari anime tersebut berbeda, ini menandakan bahwa jenis berbeda walaupun tetap satu anime yang sama. Jadi dari indikasi sebelumnya dapat disimpulkan bahwa data tidak memiliki keanehan karena walaupun memiliki judul yang sama, namun berbeda jenis.

### Data Preprocessing

Dilakukan penggabungan data berdasarkan kesamaan anime_id dan mengubah nama kolom rating pada rating_df menjadi rating_user.
"""

alldata_df = pd.merge(anime_df, rating_df.rename(columns={'rating': 'rating_user'}), on='anime_id')
alldata_df.head()

"""**Penjelasan:**
Data rating_df dan anime_df telah digabungkan berdasarkan anime_id dan disimpan dalam variabel bernama alldata_df.

### Mempersiapkan Data untuk Model Content Based Filtering

#### **1. Pengecekan deskripsi alldata_df**
"""

alldata_df.info()

alldata_df.describe()

"""**Penjelasan:**
Terlihat bahwa jumlah data untuk setiap fitur sama dan pada fitur rating minimalnya adalah 1.67 dan maksimalnya 9.5, namun pada fitur rating_user, minimalnya adalah -1 dan maksimalnya adalah 10 dengan -1 berarti user tersebut tidak memberikan rating untuk anime yang ditonton. Ini bukan suatu permasalahan, karena fitur rating adalah rate untuk judul anime dengan type tertentu secara keseluruhan, sedangkan rating_user adalah rating yang diberikan setiap user untuk anime yang ditonton.

#### **2. Pengecekan missing value dan duplikat pada alldata_df**
"""

print("Missing value di alldata_df:")
print(alldata_df.isnull().sum())

print("\nDuplicate data di alldata_df:")
print(alldata_df.duplicated().sum())

"""**Penjelasan:**
Terlihat bahwa alldata_df tidak memiliki missing value dan data yang duplikat.

#### **3. Pengecekan jenis genre dari data yang telah digabungkan**
"""

genres = alldata_df['genre'].str.split(', ').explode().unique()
print("Total unik genre:", len(genres))
print("Daftar genre yang ada secara keseluruhan:", list(genres))

"""**Penjelasan:**
Terlihat bahwa jenis genre tidak memiliki keanehan dan sudah benar. Perbedaan yang ada hanya jumlah genre pada data yang telah dilakukan penggabungan serta penanganan missing value dan duplikat berjumlah 43, namun jumlah genre di data awal anime_df adalah 44. Ini tidak memiliki indikasi adanaya keanehan, karena setelah dilakukan penanganan, maka beberapa data awal telah dil;akukan penghapusan yang mengakibatkan jumlah genrenya menjadi berkurang 1.

#### **4. Mengambil data dari user dengan minimal nonton 1000 anime dan rating >= 7**

Untuk mendapatkan rekomendasi yang lebih baik, akan lebih baik mengambil data hanya dari user dengan pengetahuan anime cukup mumpuni. Ini dapat dilihat dari seberapa banyak user tersebut menonton dan memberi rating pada anime yang ditonton. Akan diambil user dengan minimal 1000 interaksi yang dimana ini akan memperkuat kemungkinan bahwa user tersebut sudah menonton banyak jenis anime dari berbagai genre. Hal ini dilakukan karena dimisalkan jika hanya mengambil user dengan jumlah interaksi < 100, maka ada kemungkinan bahwa user tersebut hanya menonton 2 sampai 3 anime dan interaksi tersebut dihasilkan dari setiap episode yang dia rating. Untuk lebih memberikan rekomendasi yang lebih baik, maka hanya akan diambil data dengan rating >= 7. Ini dilakukan agar anime yang direkomendasikan memiliki nilai yang cukup dikatakan bagus dan layak untuk direkomendasikan.
"""

min_interactions = 1000
min_rating = 7

# Filter user yang menonton minimal 500 anime
user_counts = alldata_df['user_id'].value_counts()
filtered_users = user_counts[user_counts >= min_interactions].index

# Filter alldata_df untuk user yang memenuhi kriteria jumlah tontonan
filtered_alldata_df = alldata_df[alldata_df['user_id'].isin(filtered_users)]

# Filter anime dengan rating >= 7 (rating keseluruhan anime)
anime_with_high_rating = filtered_alldata_df[filtered_alldata_df['rating'] >= min_rating]

# Filter data untuk user yang memberikan rating >= 7 (rating user) untuk anime
fix_data_df = anime_with_high_rating[anime_with_high_rating['rating_user'] >= min_rating]

# Tampilkan informasi mengenai fix_data_df
print("\nInfo of fix_data_df:")
fix_data_df.info()
print("\nFirst 5 rows of fix_data_df:")
fix_data_df

"""**Penjelasan:**
Setelah dilakukan filtering berdasarkan jumlah kemunculan user dan ratingnya, maka dataset berjumlah 134306.

#### **5. Mengonversi data series menjadi list**

Untuk membuat rekomendasi berdasarkan genre, maka akan disiapkan datanya yang mencakup fitur name, genre, type dan episodes.
"""

# Mengonversi data series ‘name’ menjadi dalam bentuk list
name_df = fix_data_df['name'].tolist()

# Mengonversi data series ‘genre’ menjadi dalam bentuk list
genre_df = fix_data_df['genre'].tolist()

# Mengonversi data series ‘type’ menjadi dalam bentuk list
type_df = fix_data_df['type'].tolist()

# Mengonversi data series ‘type’ menjadi dalam bentuk list
episodes_df = fix_data_df['episodes'].tolist()

print("Jumlah data name_df:", len(name_df))
print("Jumlah data genre_df:", len(genre_df))
print("Jumlah data type_df:", len(type_df))
print("Jumlah data episodes_df:", len(episodes_df))

"""**Penjelasan:**
Setelah membuat list untuk setiap fitur yang akan digunakan, terlihat bahwa jumlah data masing-masing fitur berjumlah 134306.

#### **6. Membuat dictionary untuk rekomendasi berdasarkan genre**
"""

# Membuat dictionary untuk rekomendasi berdasarkan genre
genre_base = pd.DataFrame({
    'judul': name_df,
    'genre': genre_df,
    'jenis': type_df,
    'episode': episodes_df
})
genre_base

"""**Penjelasan:**
Fitur-fitur name, genre, type dan episodes sudah digabungkan dalam dataframe bernama genre_base dan berjumlah 134306 data.

#### **7. Pengecekan dan penanganan duplikat dari dataframe baru yakni genre_base yang akan dirujuk untuk membuat rekomendasi**
"""

print("\nDuplicate data di genre_base:")
print(genre_base.duplicated().sum())

"""**Penjelasan:**
Terlihat bahwa genre_base memiliki sangat banyak data duplikat yakni sebanyak 130951. Ini dikarenakan data sebelumnya memiliki fitur user_id dan rating yang memungkinkan ada banyak sekali user yang memberi rating pada satu anime dan episode yang sama. Hal itu tidak dikategorikan menjadi data duplikat karena penonton atau usernya berbeda. Adapula fitur rating, jikalau user memberikan rating yang berbeda pada satu anime dan episode yang sama, maka ini tetap tidak dapat dikatakan duplikat karena perbedaan dari rating yang diberikan. Namun pada dataframe genre_base, hanya diambil fitur judul, genre, jenis dan episode atau pada data sebelumnya bernama name, genre, type, episodes. Maka pastinya data memiliki banyak duplikat karena user_id dan rating telah dihilangkan.
"""

genre_base = genre_base.drop_duplicates()
print("\nDuplicate data di genre_base:")
print(genre_base.duplicated().sum())
print("\nInfo of genre_base after dropping duplicates:")
genre_base.info()
print("\nFirst 5 rows of genre_base after dropping duplicates:")
genre_base.head()

"""**Penjelasan:**
Setelah penanganan duplikat, maka data menjadi berjumlah 3355.

#### **8. TF-IDF Vectorizer**
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Menangani jika genre kosong
genre_base['genre'] = genre_base['genre'].fillna('')

tfidf = TfidfVectorizer(tokenizer=lambda x: x.split(', '))

# Melakukan fit_transform pada data genre
tfidf_matrix = tfidf.fit_transform(genre_base['genre'])

# Melihat shape dari tfidf_matrix
tfidf_matrix.shape

"""**Penjelasan:**
Terlihat bahwa matriks yang dimiliki berukuran (3355, 43). Nilai 3355 merupakan ukuran data dan 43 merupakan matrik genre.
"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""**Penjelasan:**
Vektor tf-idf telah diubah dalam bentuk matriks tf-idf.
"""

# Membuat dataframe untuk melihat tf-idf matrix
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf.get_feature_names_out(),
    index=genre_base[['judul', 'episode', 'jenis']].apply(tuple, axis=1)
).sample(10, axis=0).sample(43, axis=1) # Tuple untuk index unik

"""**Penjelasan:**
Output matriks tfidf ini adalah kecocokan judul, episode dan type berdasarkan genre. Contohnya adalah (Kirarin☆Revolution, 153, TV) yang memiliki nilai 0.443141 pada genre drama. Ini mengandung arti bahwa (Kirarin☆Revolution, 153, TV) memiliki genre drama dan seterusnya.

### Mempersiapkan Data untuk Model Collaborative Filtering

Pada proses sebelumnya, kita sudah memiliki data yang telah dilakukan penggabungan dengan nama alldata_df. Maka akan dilakukan preparation data dari data tersebut.

#### **1. Menghapus rating_user < 0 dan mengambil data dari user dengan minimal interaksi 1000**

Pada sebelumnya, kita ketahui bahwa rating_user -1 adalah rating yang digeneralisasi untuk user yang nonton anime namun tidak memberikan rating. Maka hal itu tidak akan dijadikan acuan dalam rekomendasi dan akan dihapus.
"""

alldata_df = alldata_df[alldata_df['rating_user'] >= 0]
print("\nInfo of alldata_df after removing rating_user < 0:")
alldata_df.describe()

"""**Penjelasan:**
Terlihat bahwa rating_user setelah dilakukan penanganan memiliki minimal rating 1.

Jika diambil data walaupun hanya 1 interaksi, maka itu akan kurang untuk rekomendasi. Hal ini dikarenakan data yang diperoleh dari user tersebut kurang merepresentasikan apa yang disukai oleh user karena hanya menonton 1 episode anime. Itu akan mengurangi akurasi model. Kenyataannya, biasanya dalam 1 anime memiliki sekitar 24 episode untuk 1 season dan biasanya memiliki beberapa season. Jikalau user tersebut hanya melakukan interaksi terhadap 1 anime dengan beebrapa season, maka itu akan tetap kurang merepresentasikan data dari user tersebut. Maka akan diambil data dari user dengan minimal interaksi 1000.
"""

# Mengambil data dengan interaksi user minimal 1000
user_counts = alldata_df['user_id'].value_counts()
filtered_users = user_counts[user_counts >= 1000].index
alldata_df = alldata_df[alldata_df['user_id'].isin(filtered_users)]

print("Jumlah data dari alldata_df:", len(alldata_df))

"""**Penjelasan:**
Setelah hanya diambil user dengan minimal 1000 interaksi dan menghapus rating_user < 0, maka jumlah data menjadi 218718.

#### **2. Mengambil user_id unik dan melakukan encoding**
"""

# Mengubah user_id menjadi list tanpa nilai yang sama
user_ids = alldata_df['user_id'].unique().tolist()
print('list user_id: ', user_ids)

# Melakukan encoding user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded user_id : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke user_id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke user_id: ', user_encoded_to_user)

"""**Penjelasan:**
user_id telah dipetakan kedalam integer 0 sampai dengan banyaknya jumlah user_id. Pada kali ini user_id telah berhasil dilakukan encoding dengan contoh user_id 1497 menjadi 0 dan seterusnya.

#### **3. Mengambil judul atau name unik dan melakukan encoding**
"""

# Mengubah name menjadi list tanpa nilai yang sama
name_ids = alldata_df['name'].unique().tolist()
print('list name: ', name_ids)

# Melakukan proses encoding name
name_to_encoded = {x: i for i, x in enumerate(name_ids)}
print('encoded name : ', name_to_encoded)

# Melakukan proses encoding angka ke name
name_encoded_to_name = {i: x for i, x in enumerate(name_ids)}
print('encoded angka ke name: ', name_encoded_to_name)

"""**Penjelasan:**
name telah dipetakan kedalam integer 0 sampai dengan banyaknya jumlah judul. Pada kali ini name telah berhasil dilakukan encoding dengan contoh name Kimi no Na wa. menjadi 0 dan seterusnya.

#### **4. Memetakan user_id dan name ke dataframe yang berkaitan**
"""

# Mapping user_id ke dataframe user
alldata_df['user'] = alldata_df['user_id'].map(user_to_user_encoded)

# Mapping name ke dataframe judul
alldata_df['judul'] = alldata_df['name'].map(name_to_encoded)

"""**Penjelasan:**
Dilakukan mapping pada user_id dan name dengan diberi nama fitur berturut-turut yakni user dan judul.

#### **5. Cek jumlah user, jumlah judul, dan mengubah nilai rating_user menjadi float**
"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah judul
num_judul = len(name_encoded_to_name)
print(num_judul)

# Mengubah rating menjadi nilai float
alldata_df['rating_user'] = alldata_df['rating_user'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(alldata_df['rating_user'])

# Nilai maksimal rating
max_rating = max(alldata_df['rating_user'])

print('Number of User: {}, Number of Judul: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_judul, min_rating, max_rating
))

"""**Penjelasan:**
Mengecek beberapa hal dalam data seperti jumlah user, jumlah judul, kemudian mengubah nilai rating_user menjadi float.

#### **6 Mempersiapkan data untuk rekomendasi**
"""

anime_df_fix = alldata_df.copy()

# Mendapatkan data user yang belum pernah ditonton
user_id = alldata_df.user_id.sample(1).iloc[0]
watched_anime_by_user = anime_df_fix[anime_df_fix.user_id == user_id]

# Mengambil data anime yang belum ditonton user
unwatched_anime_df = anime_df_fix[~anime_df_fix['anime_id'].isin(watched_anime_by_user.anime_id.values)][
    ['anime_id', 'name']
]
unwatched_anime_df = unwatched_anime_df.drop_duplicates(subset=['anime_id'])

unwatched_anime_df = unwatched_anime_df.sample(frac=1, random_state=42)
unwatched_anime_df = unwatched_anime_df.head(10000)

# Melakukan encoding anime_id
unwatched_anime_df['judul'] = unwatched_anime_df['anime_id'].map(name_to_encoded)

# Mengambil semua data user
user_anime_array = np.hstack(
    ([[user_to_user_encoded.get(user_id)]] * unwatched_anime_df.shape[0],
     unwatched_anime_df['judul'].values.reshape(-1, 1))
)

"""**Penjelasan:**
Dari data kita ketahui bahwa user telah memberi rating pada beberapa judul anime yang telah mereka tonton. Rating ini digunakan untuk membuat rekomendasi anime yang mungkin cocok untuk user. Judul yang akan direkomendasikan tentulah judul yang belum pernah ditonton oleh user. Maka dari itu, disini dibuat data anime yang belum pernah ditonton oleh setiap user dan melakukan encoding terhadap anime_id.

#### **7. Membagi data**
"""

# Mengacak dataset
alldata_df = alldata_df.sample(frac=1, random_state=42)
alldata_df

"""**Penjelasan:**
Mengacak dataset.
"""

# Membuat variabel x untuk mencocokkan data user dan judul menjadi satu value
x = alldata_df[['user', 'judul']].values

# Membuat variabel y untuk membuat rating_user dari hasil
y = alldata_df['rating_user'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80  % data train dan 20% data validasi
train_indices = int(0.8 * alldata_df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""**Penjelasan:**
Split dataset dengan metode 80:20 dimana 80% untuk data train dan 20% untuk data validasi. Membuat variabel x yang berisi data user dan judul serta variabel y yang berisi data rating_user.

## **Modeling**

### **Modeling dengan Content Based Filtering**

#### **1. Cosine Similarity**
"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul
cosine_sim_df = pd.DataFrame(cosine_sim, index=genre_base['judul'], columns=genre_base['judul'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap judul
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""**Penjelasan:**
Dari matriks tfidf, dibuat matriks cosine similarity dimana matriks ini digunakan untuk mengukur kesamaan setiap anime dengan anime lain. Jumlah Shape: (3355, 3355) memiliki arti bahwa matriks cosine similarity memiliki ukuran 3355*3355 dengan 3355 adalah jumlah judul anime.

#### **2. Membuat fungsi rekomendasi judul anime berdasarkan kesamaan genre untuk Content Based Filtering**
"""

def anime_recommendations(judul, similarity_data=cosine_sim_df, items=genre_base[['judul', 'genre']], k=10):

    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    index = similarity_data.loc[:,judul].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop judul yang dicari agar judul yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(judul, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""**Penjelasan:**
Dibuat fungsi untuk rekomendasi top 10 anime berdasarkan kesanaab genre dan menghapus nama judul yang dicari dari dataframe hasil rekomendasi.

#### **3. Mendapatkan rekomendasi**
"""

genre_base[genre_base.judul.eq('Death Note')]

# Mendapatkan rekomendasi judul yang mirip dengan Death Note
anime_recommendations('Death Note')

"""**Penjelasan:**
Hasil diatas adalah hasil top 10 anime yang memiliki kesamaan genre dengan Death Note.

### **Modeling dengan Collaborative Filtering**

#### **1. Menghitung kecocokan user dan judul dengan teknik embedding**
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_judul, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_judul = num_judul
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.judul_embedding = layers.Embedding( # layer embeddings judul
        num_judul,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.judul_bias = layers.Embedding(num_judul, 1) # layer embedding judul bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    judul_vector = self.judul_embedding(inputs[:, 1]) # memanggil layer embedding 3
    judul_bias = self.judul_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_judul = tf.tensordot(user_vector, judul_vector, 2)

    x = dot_user_judul + user_bias + judul_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""**Penjelasan:**
Melakukan proses embedding terhadap data user dan judul. Selanjutnya, dilakukan operasi perkalian dot product antara embedding user dan judul. Ditambahkan bias untuk setiap user dan judul pula dan hasil skor kecocokan ditetapkan dalam skala [0,1] dengan fungsi aktivasi sigmoid. Selanjutnya, membuat class RecommenderNet pula dengan keras Model class.

#### **2. Mendefinisikan model**
"""

model = RecommenderNet(num_users, num_judul, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Buat callback dengan early stopping
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('root_mean_squared_error') < 0.1):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True

callbacks = myCallback()

"""**Penjelasan:**
Model dibuat menggunakan Binary Crossentropy untuk menghitung loss function, Adam (Adaptive Moment Estimation) sebagai optimizer dengan learning rate 0.001, dan root mean squared error (RMSE) sebagai metrics evaluation. Diterapkan callbacks pula untuk model ketika tingkat akurasinya sudah diatas 90%.

#### **3. Training model**
"""

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val),
    callbacks=[callbacks]
)

"""**Penjelasan:**
Terlihat bahwa model terhenti di epoch 100 karena nilai akurasi kurang dari 90%, maka dari itu callback tidak dijalankan.

#### **4. Mendapatkan rekomendasi**
"""

predictions = model.predict(user_anime_array).flatten()

top_predictions = predictions.argsort()[-10:][::-1]
recommended_anime_ids = unwatched_anime_df.iloc[top_predictions]['anime_id'].values

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Anime with high ratings from user')
print('----' * 8)

top_rated_anime_by_user = watched_anime_by_user.sort_values(
    by='rating_user', ascending=False
).head(5)

for row in top_rated_anime_by_user.itertuples():
    print(row.name, ':', row.genre)

print('----' * 8)
print('Top 10 Anime Recommendations')
print('----' * 8)

recommended_anime = anime_df[anime_df['anime_id'].isin(recommended_anime_ids)]

for row in recommended_anime.itertuples():
    print(row.name, ': ', row.genre, ' (Type: ', row.type, ', Episodes: ', row.episodes, ')')

"""**Penjelasan:**
Hasil top 10 rekomendasi collaborative dari user 58064 berdasarkan rating yang dilakukannya.

## **Evaluation**

#### **1. Evaluasi metrik error Content Based Filtering**

Pada model yang dibuat untuk pendekatan Content Based Filtering, kita ketahui bahwa model hanya bergantung pada kemiripan genre. Maka pada kali ini, akan dilakukan metrik evaluasi precision dengan berdasarkan perhitungan hitungan kesamaan jaccard pada genre. Jacard sendiri akan melihat seberapa persen atau besar kesamaan genre antara setiap anime yang direkomendasikan dengan anime target yang pada kali ini adalah Death Note. Nantinya hasil tersebut akan dihitung evluasinya dengan menggunakan precision dengan treshold nilai jaccard 0.5. Ini mengandung arti, jika nilai jaccard pada suatu anime kurang dari 0.5, maka itu akan termasuk false pada precision, begitupun sebaliknya.
"""

def jaccard_similarity(set1, set2):
    return len(set1.intersection(set2)) / len(set1.union(set2))

target_genres = set(genre_base.loc[genre_base['judul'] == 'Death Note', 'genre'].values[0].split(', '))
rekom = anime_recommendations('Death Note', k=10)

# Hitung Jaccard similarity tiap rekomendasi
rekom['jaccard'] = rekom['genre'].apply(lambda g: jaccard_similarity(set(g.split(', ')), target_genres))

# Threshold untuk dianggap relevan
threshold = 0.5
relevan_count = (rekom['jaccard'] >= threshold).sum()

precision = relevan_count / len(rekom)
print(f'Precision@10 with Jaccard threshold {threshold}: {precision:.2f}')

"""**Penjelasan:**
Dari hasil evaluasi, diperoleh bahwa model yang telah dibuat memiliki nilai akurasi 0.8. Ini sudah sangat baik untuk suatu sistem rekomendasi.

#### **2. Visualisasi metrik error untuk Collaborative Filtering**
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Penjelasan:**
Proses training model cukup smooth dan model konvergen pada epochs 100. Dari proses ini, kita memperoleh nilai error RMSE akhir sebesar 0.1309 pada train dan 0.1380 pada validasi. Nilai tersebut cukup bagus untuk sistem rekomendasi.
"""